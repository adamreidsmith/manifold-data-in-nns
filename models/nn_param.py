#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue May 28 12:32:10 2019

@author: adamreidsmith
"""

'''
Neural network to predict parameters a and b given a solution to the 
Van der Pol equation: x'' = a*(1 - x^2)*x' - b*y + f(t).

Datafiles for this network can be generated by running 'vdp_param.py'.
'''

from os import path
assert path.exists('./datafiles/vdp_param_data.npy'), \
       'Datafile not found. Please run \'vdp_param.py\' to generate a datafile.'
import torch
import numpy as np
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt

###############################################################################
'''
Inputs:
    n_epochs:       Number of epochs to train for.
    
    batch_size:     Batch size.
    
    lr:             Learning rate.
    
    weight_decay:   Weight decay factor.
'''
###############################################################################

def main(n_epochs=50,
         batch_size=10,
         lr=0.001,
         weight_decay=1e-8):

    class Data(Dataset):
        
        def __init__(self):
            self.data = np.load('./datafiles/vdp_param_data.npy')
            
            #Parameters used in solution of Van der Pol oscillator
            self.parameters = torch.Tensor([self.data[i][0] for i in range(len(self.data))])
            
            self.len = self.parameters.shape[0]
            
            self.vdp_soln = torch.Tensor([self.data[i][1:][:,1] for i in range(len(self.data))])
                    
        def __getitem__(self, index):
            return self.vdp_soln[index], self.parameters[index]
        
        def __len__(self):
            return self.len
        
    dataset = Data()
    
    # Lengths of the training and validation datasets
    train_len = int(0.8*dataset.len)
    valid_len = dataset.len - train_len
    
    #Randomly split the data into training and validation datasets
    train_data, valid_data = random_split(dataset, (train_len, valid_len))
    
    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
    valid_loader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=True)
    
    #Model of the neural network
    class Model(nn.Module):
        
        def __init__(self):
            super(Model, self).__init__()
            
            self.l1 = nn.Linear(len(dataset.vdp_soln[0]), 500)
            self.l2 = nn.Linear(500, 50)
            self.l3 = nn.Linear(50, 2)
            
        def forward(self, x):
           
            x = torch.sigmoid(self.l1(x))
            x = torch.sigmoid(self.l2(x))
            return self.l3(x)
            
    model = Model()
    
    #Mean squared error loss
    loss_func = nn.MSELoss()
    
    #Stochastic gradient descent optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    def evaluate():
        #Evaluation mode
        model.eval()
        for data in valid_loader:
    
            #Split batch into inputs and outputs
            x, y = data[0].squeeze(0), data[1].squeeze(0)
            
            #Forward propagation
            out = model(x)
            
            #Loss computation
            loss = loss_func(out, y)
            
            #Save training loss in this batch
            valid_loss.append(loss.item())
            
            #Compute the average percent error over a validation batch
            percent_error = 100*torch.div(abs(out - y), y)
            median_percent_error.append(np.median(percent_error.detach().numpy()))
        
        return valid_loss
        
    def train():
        #Training mode
        model.train()
        for data in train_loader:
            
            #Split batch into inputs and outputs
            x, y = data[0].squeeze(0), data[1].squeeze(0)
            
            def closure():
                #Reset gradients to zero
                optimizer.zero_grad()
                
                #Forward propagation
                out = model(x)
                        
                #Loss computation
                loss = loss_func(out, y)
                
                #Backpropagation
                loss.backward()
                
                return loss
            
            #Weight optimiation
            optimizer.step(closure)
            
            #Save training loss in this batch
            train_loss.append(closure().item())
            
        return train_loss
    
    #Training and validation loop
    for epoch in range(n_epochs): #An epoch is a run of the entire training dataset
    
        train_loss, valid_loss, median_percent_error = [], [], []
        
        train_loss = train()
        
        valid_loss = evaluate()
        
        if __name__ == '__main__':
            print('Epoch', epoch+1)
            print('  Mean Epoch Training Loss:  ', np.mean(train_loss))
            print('  Mean Epoch Validation Loss:', np.mean(valid_loss))
            print('  Median percent error:      ', np.median(np.array(median_percent_error)), '%')

        if epoch % (n_epochs-1) == 0 and epoch != 0:
            #Plot a histogram of the error (predicted - true)
            error = []    
            model.eval()
            for data in valid_loader:
                #Split batch into inputs and outputs
                x, y = data[0].squeeze(0), data[1].squeeze(0)
                out = model(x)
                error.append((out - y).detach().numpy())
            
            error = np.array(error)
            a_error = np.array([error[i][j][0] for i in range(len(error)) for j in range(batch_size)])
            b_error = np.array([error[i][j][1] for i in range(len(error)) for j in range(batch_size)])
            
            plt.figure(1, figsize=(8,6))
            plt.hist(a_error, bins=30, color='b')
            plt.title('Error in parameter a')
            plt.xlabel('Predicted - True')
            plt.figure(2, figsize=(8,6))
            plt.hist(b_error, bins=30, color='k')
            plt.title('Error in parameter b')
            plt.xlabel('Predicted - True')
            plt.show()


if __name__ == '__main__':         
    main()
    
    
    


